{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importe librerias y tambien un modulo propio que llame funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from functions import convert_bin,get_dataframes,get_all_texts_for_dic\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_dataframes() = es una funcion que esta en el modulo 'funciones'.Tiene tres parametros(ruta,tipo,raiz) y trae un lista de las rutas donde se encuentran los archivos de la base de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataframes('arrl-org/arrl-2013','texto','..')\n",
    "option = df\n",
    "len(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_texts() = es una funcion que sirve para extraer los textos de cada ruta solisitados y toma de parametros una lista con rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(list_files_txt):\n",
    "    frases = []\n",
    "\n",
    "    print(len(list_files_txt))\n",
    "\n",
    "    for archivo in list_files_txt:\n",
    "        with open(archivo, 'r') as file:\n",
    "            contenido = file.read()\n",
    "            frases.append(contenido)\n",
    "            file.close()\n",
    "\n",
    "    frases_array = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        frase_modificada = frase.replace('\\n\\n', '').replace('\\n', ' ')\n",
    "        frases_array.append(frase_modificada)\n",
    "    return frases_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ‰  NOW 10 WPM  ‰  TEXT IS FROM JULY 2012 QST  PAGE 55  ‰TELEVISION WAS POPULAR THE TYPICAL TRANSMISSION LINE WAS OPEN WIRE OR TWISTED PAIR CONNECTED TO A LINK COUPLED TUNED RESONANT CIRCUIT AT THE OUTPUT OF THE TRANSMITTER, USUALLY WITH DIFFERENT PLUG IN COILS FOR EACH BAND.  BY ADJUSTING THE VARIABLE CAPACITOR, SOME ADJUSTMENT COULD BE MADE TO COMPENSATE FOR REACTANCE IN THE LOAD.  IN ADDITION, SOME HAD AN ‰  END OF 10 WPM TEXT  ‰  QST DE W1AW  ƒ \\x1a',\n",
       " ' ‰  NOW 10 WPM  ‰  TEXT IS FROM JULY 2012 QST  PAGE 75  ‰HAD A MILITARY BACKGROUND AND WAS A NATURAL LEADER WHO LED BY EXAMPLE TO PUT OUR RAGTAG TEAM TOGETHER TO GET THE JOB DONE.  I PLAYED HOOKY FROM SCHOOL TO HELP SUPPORT THE SEARCH EFFORT NOT SO MUCH OUT OF A SPIRIT OF PUBLIC SERVICE, BUT TO SEE MY FRIENDS AND PLAY RADIO IN THE FIELD.  EACH OF US WAS ASSIGNED TO THE VARIOUS DISCIPLINES OF EMERGENCY MANAGEMENT, LAW ‰  END OF 10 WPM TEXT  ‰  QST DE W1AW  ƒ \\x1a']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_texts = get_texts(option)\n",
    "array_texts[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_text() = esta funcion la hice para sacar las inperfecciones a los textos y toma de parametro una lista con strings. esta funcion no tiene return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(array_texts):\n",
    "    for i, elementos in enumerate(array_texts):\n",
    "        contador = 0\n",
    "        contador_2 = 0\n",
    "        for j, elemento in enumerate(elementos[:100]): \n",
    "            if '‰' in elemento:\n",
    "                contador += elemento.count('‰')\n",
    "            if '=' in elemento:\n",
    "                contador_2 += elemento.count('=')\n",
    "        if contador_2 > 0:\n",
    "            contador = contador_2 + 10         \n",
    "\n",
    "        if contador == 3:\n",
    "            sym_1 = elementos[:100].index('‰')\n",
    "            sym_2 = elementos[sym_1 + 1:100].index('‰')\n",
    "            sym_3 = elementos[sym_1 + sym_2 + 2 + 1:100].index('‰')\n",
    "            array_texts[i] = elementos[sym_1 + sym_2 + sym_3 + 4:]\n",
    "        elif contador == 2:\n",
    "            sym_1 = elementos[:100].index('‰')\n",
    "            sym_2 = elementos[sym_1 + 1:100].index('‰')\n",
    "            array_texts[i] = elementos[sym_1 + sym_2 + 2:]\n",
    "        elif contador == 13:\n",
    "            sym_1 = elementos[:100].index('=')\n",
    "            sym_2 = elementos[sym_1 + 1:100].index('=')\n",
    "            sym_3 = elementos[sym_1 + sym_2 + 2 + 1:100].index('=')\n",
    "            array_texts[i] = elementos[sym_1 + sym_2 + sym_3 + 4:]\n",
    "        elif contador == 12:\n",
    "            sym_1 = elementos[:100].index('=')\n",
    "            sym_2 = elementos[sym_1 + 1:100].index('=')\n",
    "            array_texts[i] = elementos[sym_1 + sym_2 + 2:]\n",
    "        else:\n",
    "            print('error',contador)\n",
    "\n",
    "    for i in range(len(array_texts)):\n",
    "        elementos = array_texts[i]\n",
    "        ultimos_100 = elementos[-100:]\n",
    "        if '‰' in ultimos_100:\n",
    "            indice = ultimos_100.index('‰')\n",
    "            elementos = elementos[:len(elementos) - 100 + indice]\n",
    "            array_texts[i] = elementos\n",
    "        elif '=' in ultimos_100:\n",
    "            indice = ultimos_100.index('=')\n",
    "            elementos = elementos[:len(elementos) - 100 + indice]\n",
    "            array_texts[i] = elementos\n",
    "        else:\n",
    "            print('not found')\n",
    "\n",
    "    for i in array_texts:\n",
    "        i.lstrip()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(array_texts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lista con strings limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OF STATIONS FROM ONE OF THE FOUR AREAS.  7.  IT IS INTERESTING TO EXPERIENCE HOW OFTEN EITHER CASUAL DX OR SOMETIMES BIG TIME DXPEDITIONS OPERATING FROM SOUTHEAST ASIA OR THE CENTRAL PACIFIC TRY TO IGNORE OR LIMIT VK/ZL CALLS ON THE BASIS THAT THEY CAN ALWAYS WORK THIS LOCATION SO WE WILL PLACE THEM ON HOLD AND WORK THEM WHEN WE HAVE TIME.  THEY SIMPLY FAIL TO ',\n",
       " 'EXAMPLE, CALLING VK/ZL AT 0300 UTC IS NOT A VERY PRODUCTIVE IDEA BUT I HAVE HEARD A LOT OF DX TRY.  ONE OF THE OPERATING HABITS NOW REGULARLY DISPLAYED BY VK/ZL DXERS IS TO CALL THE DXPEDITION IRRESPECTIVE OF WHETHER HE IS CALLING FOR A SPECIFIC AREA OR NOT.  NOW, THIS MAY WELL BE DOWNRIGHT RUDENESS BY THE VK/ZL, AND WILL CERTAINLY BE SEEN AS SUCH BY SOME DXERS, ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_texts[3:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importe algunas funciones nuevas del modulo 'funciones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import get_dataframes,covert_dic_to_json,get_all_texts_for_dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "este es un bucle con un try para manejar errores donde:\n",
    "1. consumo la primera ruta\n",
    "2. busco la lista de todas las rutas de ese año\n",
    "3. la funcion get_all_text_for_dic tiene integrada get_texts(extrae textos de las rutas) y clean_text(limpia en caso de ser nesecario) y luego las tranforma a diccionario con la keys=arrl-{year} y los values=texto limpio \n",
    "4. y por ultimo covert_dic_to_json convierte el diccionario a json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2013, 2023):\n",
    "    try:\n",
    "        path_folder = f'arrl-org/arrl-{year}/'\n",
    "        # path_folder = 'F_10-16x16_Koch_Training'\n",
    "        # dataframes_audio = get_dataframes(path_folder,'audio')\n",
    "        dataframes_texto = get_dataframes(path_folder,'texto','..')\n",
    "        get_dic_textos = get_all_texts_for_dic(dataframes_texto)\n",
    "        # get_dic_audio = get_all_binarys_for_dic(dataframes_audio)\n",
    "        covert_dic_to_json(get_dic_textos,f\"{path_folder.rstrip('/').split('/')[-1]}-texto\")    \n",
    "    except Exception as e:\n",
    "        print(\"error:\", str(e))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorizar output(textos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- esto se hace para que funcione mejor en la red  . A todas las fraces las divide por palabras , esto quiere decir que la matriz del output va a ser de (muestras totales, palabras totales contando todos los ejemplos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 2853)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "matriz_conteo = vectorizer.fit_transform(array_texts)\n",
    "\n",
    "print(matriz_conteo.toarray().shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso invero a vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['television', 'was', 'popular', 'the', 'typical', 'transmission',\n",
      "       'line', 'open', 'wire', 'or', 'twisted', 'pair', 'connected', 'to',\n",
      "       'link', 'coupled', 'tuned', 'resonant', 'circuit', 'at', 'output',\n",
      "       'of', 'transmitter', 'usually', 'with', 'different', 'plug', 'in',\n",
      "       'coils', 'for', 'each', 'band', 'by', 'adjusting', 'variable',\n",
      "       'capacitor', 'some', 'adjustment', 'could', 'be', 'made',\n",
      "       'compensate', 'reactance', 'load', 'addition', 'had', 'an'],\n",
      "      dtype='<U16'), array(['was', 'the', 'to', 'of', 'in', 'each', 'by', 'had', 'military',\n",
      "       'background', 'and', 'natural', 'leader', 'who', 'led', 'example',\n",
      "       'put', 'our', 'ragtag', 'team', 'together', 'get', 'job', 'done',\n",
      "       'played', 'hooky', 'from', 'school', 'help', 'support', 'search',\n",
      "       'effort', 'not', 'so', 'much', 'out', 'spirit', 'public',\n",
      "       'service', 'but', 'see', 'my', 'friends', 'play', 'radio', 'field',\n",
      "       'us', 'assigned', 'various', 'disciplines', 'emergency',\n",
      "       'management', 'law'], dtype='<U16')]\n"
     ]
    }
   ],
   "source": [
    "vectorizar = CountVectorizer()\n",
    "\n",
    "matriz_conteo = vectorizar.fit_transform(array_texts)\n",
    "\n",
    "strings_decodificados = vectorizar.inverse_transform(matriz_conteo)\n",
    "\n",
    "print(strings_decodificados[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AERO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
